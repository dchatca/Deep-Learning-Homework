{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9858ed2f",
   "metadata": {},
   "source": [
    "# Nguyễn Ngọc Đạt - 11200745 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd8a11e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ffb56e",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a84092cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6c0f5",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6e8aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=64, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "949c93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3aef601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar_10_pre_model = models.vgg11(pretrained = True)\n",
    "#cifar_10_pre_model.load_state_dict(torch.load(\"cifar10_mini_vgg.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c67926",
   "metadata": {},
   "source": [
    "# Model and Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58076575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVGG(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(MiniVGG, self).__init__()\n",
    "        self.features = nn.Sequential(nn.Conv2d(1,64,kernel_size=3,padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                                      \n",
    "                                      nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(128,128,kernel_size=3,padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                                      \n",
    "                                      nn.Conv2d(128,256,kernel_size=3,padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                                     \n",
    "                                      \n",
    "        )\n",
    "        self.classifier = nn.Linear(256*3*3,10)\n",
    "        nn.init.normal_(self.classifier.weight,0, 0.01)\n",
    "        nn.init.constant_(self.classifier.bias,0)\n",
    "    \n",
    "    \n",
    "    def forward(self,x,):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b058e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniVGG().to(device)\n",
    "model.load_state_dict(torch.load(\"cifar10_mini_vgg.pth\", map_location=torch.device('cpu')))\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c796921",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = MiniVGG().to(device)\n",
    "model_2.load_state_dict(torch.load('mnist_mini_vgg.pth',map_location=torch.device('cpu')))\n",
    "for param in model_2.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_2.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c395ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = MiniVGG().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c1501dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniVGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=2304, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27932db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniVGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=2304, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e2e149c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniVGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=2304, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "039b2355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Step: 100/938, Loss: 0.9675\n",
      "Epoch: 1/5, Step: 200/938, Loss: 1.6599\n",
      "Epoch: 1/5, Step: 300/938, Loss: 0.4458\n",
      "Epoch: 1/5, Step: 400/938, Loss: 0.8714\n",
      "Epoch: 1/5, Step: 500/938, Loss: 0.3587\n",
      "Epoch: 1/5, Step: 600/938, Loss: 0.4434\n",
      "Epoch: 1/5, Step: 700/938, Loss: 0.5062\n",
      "Epoch: 1/5, Step: 800/938, Loss: 0.7080\n",
      "Epoch: 1/5, Step: 900/938, Loss: 0.4727\n",
      "Epoch: 2/5, Step: 100/938, Loss: 0.4045\n",
      "Epoch: 2/5, Step: 200/938, Loss: 0.3888\n",
      "Epoch: 2/5, Step: 300/938, Loss: 0.3312\n",
      "Epoch: 2/5, Step: 400/938, Loss: 0.2357\n",
      "Epoch: 2/5, Step: 500/938, Loss: 0.4038\n",
      "Epoch: 2/5, Step: 600/938, Loss: 0.4527\n",
      "Epoch: 2/5, Step: 700/938, Loss: 0.3560\n",
      "Epoch: 2/5, Step: 800/938, Loss: 0.3118\n",
      "Epoch: 2/5, Step: 900/938, Loss: 0.4891\n",
      "Epoch: 3/5, Step: 100/938, Loss: 0.4354\n",
      "Epoch: 3/5, Step: 200/938, Loss: 0.2841\n",
      "Epoch: 3/5, Step: 300/938, Loss: 0.3805\n",
      "Epoch: 3/5, Step: 400/938, Loss: 0.1917\n",
      "Epoch: 3/5, Step: 500/938, Loss: 0.4397\n",
      "Epoch: 3/5, Step: 600/938, Loss: 0.3439\n",
      "Epoch: 3/5, Step: 700/938, Loss: 0.3759\n",
      "Epoch: 3/5, Step: 800/938, Loss: 0.2873\n",
      "Epoch: 3/5, Step: 900/938, Loss: 0.3538\n",
      "Epoch: 4/5, Step: 100/938, Loss: 0.4879\n",
      "Epoch: 4/5, Step: 200/938, Loss: 0.2746\n",
      "Epoch: 4/5, Step: 300/938, Loss: 0.2082\n",
      "Epoch: 4/5, Step: 400/938, Loss: 0.3441\n",
      "Epoch: 4/5, Step: 500/938, Loss: 0.3487\n",
      "Epoch: 4/5, Step: 600/938, Loss: 0.3788\n",
      "Epoch: 4/5, Step: 700/938, Loss: 0.2518\n",
      "Epoch: 4/5, Step: 800/938, Loss: 0.3755\n",
      "Epoch: 4/5, Step: 900/938, Loss: 0.4818\n",
      "Epoch: 5/5, Step: 100/938, Loss: 0.3172\n",
      "Epoch: 5/5, Step: 200/938, Loss: 0.4842\n",
      "Epoch: 5/5, Step: 300/938, Loss: 0.3587\n",
      "Epoch: 5/5, Step: 400/938, Loss: 0.1494\n",
      "Epoch: 5/5, Step: 500/938, Loss: 0.1031\n",
      "Epoch: 5/5, Step: 600/938, Loss: 0.2025\n",
      "Epoch: 5/5, Step: 700/938, Loss: 0.2485\n",
      "Epoch: 5/5, Step: 800/938, Loss: 0.3892\n",
      "Epoch: 5/5, Step: 900/938, Loss: 0.4102\n",
      " Accuracy: 0.8864\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Set the batch size of the model\n",
    "#model.batch_size = 3\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        #loss.requires_grad = True\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #loss.requires_grad = True\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Epoch: {}/5, Step: {}/{}, Loss: {:.4f}\".format(\n",
    "                epoch + 1, i + 1, len(train_loader), loss.item()\n",
    "            ))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f' Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "04527ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Step: 100/938, Loss: 1.1490\n",
      "Epoch: 1/5, Step: 200/938, Loss: 0.4822\n",
      "Epoch: 1/5, Step: 300/938, Loss: 1.0779\n",
      "Epoch: 1/5, Step: 400/938, Loss: 0.4449\n",
      "Epoch: 1/5, Step: 500/938, Loss: 0.5311\n",
      "Epoch: 1/5, Step: 600/938, Loss: 0.4766\n",
      "Epoch: 1/5, Step: 700/938, Loss: 0.5198\n",
      "Epoch: 1/5, Step: 800/938, Loss: 0.4444\n",
      "Epoch: 1/5, Step: 900/938, Loss: 0.4142\n",
      "Epoch: 2/5, Step: 100/938, Loss: 0.3893\n",
      "Epoch: 2/5, Step: 200/938, Loss: 0.3088\n",
      "Epoch: 2/5, Step: 300/938, Loss: 0.4858\n",
      "Epoch: 2/5, Step: 400/938, Loss: 0.7726\n",
      "Epoch: 2/5, Step: 500/938, Loss: 0.3890\n",
      "Epoch: 2/5, Step: 600/938, Loss: 0.4285\n",
      "Epoch: 2/5, Step: 700/938, Loss: 0.3552\n",
      "Epoch: 2/5, Step: 800/938, Loss: 0.4504\n",
      "Epoch: 2/5, Step: 900/938, Loss: 0.3976\n",
      "Epoch: 3/5, Step: 100/938, Loss: 0.3937\n",
      "Epoch: 3/5, Step: 200/938, Loss: 0.4769\n",
      "Epoch: 3/5, Step: 300/938, Loss: 0.4749\n",
      "Epoch: 3/5, Step: 400/938, Loss: 0.2886\n",
      "Epoch: 3/5, Step: 500/938, Loss: 0.5102\n",
      "Epoch: 3/5, Step: 600/938, Loss: 0.3737\n",
      "Epoch: 3/5, Step: 700/938, Loss: 0.3354\n",
      "Epoch: 3/5, Step: 800/938, Loss: 0.3087\n",
      "Epoch: 3/5, Step: 900/938, Loss: 0.3587\n",
      "Epoch: 4/5, Step: 100/938, Loss: 0.4083\n",
      "Epoch: 4/5, Step: 200/938, Loss: 0.4370\n",
      "Epoch: 4/5, Step: 300/938, Loss: 0.4388\n",
      "Epoch: 4/5, Step: 400/938, Loss: 0.3482\n",
      "Epoch: 4/5, Step: 500/938, Loss: 0.4130\n",
      "Epoch: 4/5, Step: 600/938, Loss: 0.4232\n",
      "Epoch: 4/5, Step: 700/938, Loss: 0.3420\n",
      "Epoch: 4/5, Step: 800/938, Loss: 0.3288\n",
      "Epoch: 4/5, Step: 900/938, Loss: 0.3942\n",
      "Epoch: 5/5, Step: 100/938, Loss: 0.3382\n",
      "Epoch: 5/5, Step: 200/938, Loss: 0.3068\n",
      "Epoch: 5/5, Step: 300/938, Loss: 0.3500\n",
      "Epoch: 5/5, Step: 400/938, Loss: 0.5311\n",
      "Epoch: 5/5, Step: 500/938, Loss: 0.4426\n",
      "Epoch: 5/5, Step: 600/938, Loss: 0.4567\n",
      "Epoch: 5/5, Step: 700/938, Loss: 0.3303\n",
      "Epoch: 5/5, Step: 800/938, Loss: 0.2570\n",
      "Epoch: 5/5, Step: 900/938, Loss: 0.4769\n",
      "Accuracy: 0.8457\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Set the batch size of the model\n",
    "model_2.batch_size = 3\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_2(images)\n",
    "        #loss.requires_grad = True\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #loss.requires_grad = True\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Epoch: {}/5, Step: {}/{}, Loss: {:.4f}\".format(\n",
    "                epoch + 1, i + 1, len(train_loader), loss.item()\n",
    "            ))\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "model_2.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        outputs = model_2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9af689e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Step: 100/938, Loss: 2.3002\n",
      "Epoch: 1/5, Step: 200/938, Loss: 2.3010\n",
      "Epoch: 1/5, Step: 300/938, Loss: 2.2979\n",
      "Epoch: 1/5, Step: 400/938, Loss: 2.2763\n",
      "Epoch: 1/5, Step: 500/938, Loss: 2.1363\n",
      "Epoch: 1/5, Step: 600/938, Loss: 1.0979\n",
      "Epoch: 1/5, Step: 700/938, Loss: 0.7458\n",
      "Epoch: 1/5, Step: 800/938, Loss: 0.8440\n",
      "Epoch: 1/5, Step: 900/938, Loss: 0.6088\n",
      "Epoch: 2/5, Step: 100/938, Loss: 0.7156\n",
      "Epoch: 2/5, Step: 200/938, Loss: 0.6481\n",
      "Epoch: 2/5, Step: 300/938, Loss: 0.7465\n",
      "Epoch: 2/5, Step: 400/938, Loss: 0.7266\n",
      "Epoch: 2/5, Step: 500/938, Loss: 0.3460\n",
      "Epoch: 2/5, Step: 600/938, Loss: 0.6642\n",
      "Epoch: 2/5, Step: 700/938, Loss: 0.3827\n",
      "Epoch: 2/5, Step: 800/938, Loss: 0.4673\n",
      "Epoch: 2/5, Step: 900/938, Loss: 0.6610\n",
      "Epoch: 3/5, Step: 100/938, Loss: 0.5888\n",
      "Epoch: 3/5, Step: 200/938, Loss: 0.4494\n",
      "Epoch: 3/5, Step: 300/938, Loss: 0.5883\n",
      "Epoch: 3/5, Step: 400/938, Loss: 0.4256\n",
      "Epoch: 3/5, Step: 500/938, Loss: 0.4793\n",
      "Epoch: 3/5, Step: 600/938, Loss: 0.5066\n",
      "Epoch: 3/5, Step: 700/938, Loss: 0.4024\n",
      "Epoch: 3/5, Step: 800/938, Loss: 0.5700\n",
      "Epoch: 3/5, Step: 900/938, Loss: 0.5835\n",
      "Epoch: 4/5, Step: 100/938, Loss: 0.5023\n",
      "Epoch: 4/5, Step: 200/938, Loss: 0.4129\n",
      "Epoch: 4/5, Step: 300/938, Loss: 0.2558\n",
      "Epoch: 4/5, Step: 400/938, Loss: 0.5346\n",
      "Epoch: 4/5, Step: 500/938, Loss: 0.4674\n",
      "Epoch: 4/5, Step: 600/938, Loss: 0.4157\n",
      "Epoch: 4/5, Step: 700/938, Loss: 0.3225\n",
      "Epoch: 4/5, Step: 800/938, Loss: 0.2490\n",
      "Epoch: 4/5, Step: 900/938, Loss: 0.4732\n",
      "Epoch: 5/5, Step: 100/938, Loss: 0.3780\n",
      "Epoch: 5/5, Step: 200/938, Loss: 0.4998\n",
      "Epoch: 5/5, Step: 300/938, Loss: 0.3364\n",
      "Epoch: 5/5, Step: 400/938, Loss: 0.5165\n",
      "Epoch: 5/5, Step: 500/938, Loss: 0.3146\n",
      "Epoch: 5/5, Step: 600/938, Loss: 0.4608\n",
      "Epoch: 5/5, Step: 700/938, Loss: 0.6341\n",
      "Epoch: 5/5, Step: 800/938, Loss: 0.3218\n",
      "Epoch: 5/5, Step: 900/938, Loss: 0.4678\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_3.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_3(images)\n",
    "        #loss.requires_grad = True\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #loss.requires_grad = True\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Epoch: {}/5, Step: {}/{}, Loss: {:.4f}\".format(\n",
    "                epoch + 1, i + 1, len(train_loader), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1133ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.8522\n"
     ]
    }
   ],
   "source": [
    "model_3.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        outputs = model_3(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f' Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd3de5",
   "metadata": {},
   "source": [
    "### Mô hình được pretrain với tập dữ liệu ci_far10 cho ra kết quả accuracy cao nhất so với 2 mô hình còn lại. Lí do em nghĩ vì mô hình này đã được pretrain trước nên n sẽ học được đa dạng các thông tin hơn. Nếu train nhiều epochs hơn thì em nghĩ accuracy của 2 mô hình được pretrain vẫn sẽ nhỉnh hơn so với mô hình gốc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31ba6e",
   "metadata": {},
   "source": [
    "# Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b2582c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "train_nodes, eval_nodes = get_graph_node_names(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de6a3945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniVGG(\n",
       "  (features): Module(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=2304, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_feature_extractor(model_3, train_return_nodes= train_nodes, eval_return_nodes= eval_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5999bb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.3133,  0.3257,  0.0892],\n",
       "          [ 0.2212, -0.2326, -0.2578],\n",
       "          [ 0.1199, -0.2532, -0.1962]]],\n",
       "\n",
       "\n",
       "        [[[-0.1134,  0.2938,  0.1979],\n",
       "          [-0.0522,  0.2250, -0.1292],\n",
       "          [ 0.1637, -0.2342, -0.0569]]],\n",
       "\n",
       "\n",
       "        [[[-0.2860, -0.2889, -0.2345],\n",
       "          [ 0.2677, -0.0758, -0.1925],\n",
       "          [-0.0365,  0.1645, -0.1211]]],\n",
       "\n",
       "\n",
       "        [[[-0.1775,  0.2954,  0.2730],\n",
       "          [-0.0598, -0.1778,  0.0235],\n",
       "          [ 0.1670,  0.2258, -0.1984]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1088,  0.2465,  0.3005],\n",
       "          [ 0.2318, -0.0497,  0.2998],\n",
       "          [ 0.2250, -0.2388, -0.1088]]],\n",
       "\n",
       "\n",
       "        [[[-0.1128, -0.0531, -0.1690],\n",
       "          [-0.3497, -0.0665, -0.3758],\n",
       "          [-0.3149, -0.2658, -0.0942]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0211,  0.3315,  0.0070],\n",
       "          [ 0.2136, -0.2346, -0.2234],\n",
       "          [-0.1390, -0.1503,  0.2971]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1855, -0.2177,  0.0836],\n",
       "          [ 0.0762,  0.1932, -0.2714],\n",
       "          [ 0.1657, -0.1377, -0.2827]]],\n",
       "\n",
       "\n",
       "        [[[-0.1075, -0.3310, -0.2992],\n",
       "          [ 0.1641,  0.1833,  0.2042],\n",
       "          [ 0.0636, -0.0692,  0.2069]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3031, -0.1722,  0.1257],\n",
       "          [ 0.2036,  0.2229,  0.1987],\n",
       "          [ 0.0075, -0.1193, -0.3090]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2496,  0.0845, -0.0766],\n",
       "          [-0.0490,  0.2316,  0.1465],\n",
       "          [ 0.1806, -0.0418, -0.2373]]],\n",
       "\n",
       "\n",
       "        [[[-0.2572,  0.3269, -0.1307],\n",
       "          [ 0.3180, -0.2466,  0.1301],\n",
       "          [ 0.0052,  0.2861, -0.0587]]],\n",
       "\n",
       "\n",
       "        [[[-0.2732, -0.0509,  0.1845],\n",
       "          [ 0.2166,  0.0904, -0.3136],\n",
       "          [-0.0887,  0.0172,  0.1789]]],\n",
       "\n",
       "\n",
       "        [[[-0.3669, -0.0869, -0.1155],\n",
       "          [-0.3098, -0.2933,  0.0482],\n",
       "          [-0.0303, -0.0465,  0.2566]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2820,  0.2145, -0.1039],\n",
       "          [-0.3268, -0.3399,  0.3186],\n",
       "          [-0.2724, -0.3203,  0.3181]]],\n",
       "\n",
       "\n",
       "        [[[-0.1523,  0.1706,  0.0490],\n",
       "          [-0.0108,  0.2534, -0.0482],\n",
       "          [-0.2707, -0.2720,  0.1995]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2079,  0.2057,  0.2146],\n",
       "          [ 0.1250,  0.0754, -0.2934],\n",
       "          [ 0.1802,  0.2101, -0.2339]]],\n",
       "\n",
       "\n",
       "        [[[-0.0950, -0.3391, -0.2952],\n",
       "          [-0.2832, -0.3176, -0.0047],\n",
       "          [-0.2126, -0.2686, -0.2523]]],\n",
       "\n",
       "\n",
       "        [[[-0.2605, -0.2580,  0.1981],\n",
       "          [-0.1639,  0.3118,  0.3033],\n",
       "          [ 0.0283,  0.0740,  0.1515]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1277,  0.1396,  0.1070],\n",
       "          [ 0.1927, -0.0251, -0.1911],\n",
       "          [ 0.1506, -0.1179,  0.2042]]],\n",
       "\n",
       "\n",
       "        [[[-0.0609,  0.0501, -0.1842],\n",
       "          [-0.1704,  0.1808, -0.0657],\n",
       "          [ 0.1116, -0.0991, -0.2136]]],\n",
       "\n",
       "\n",
       "        [[[-0.1827,  0.2802,  0.1041],\n",
       "          [-0.0902,  0.0359, -0.2670],\n",
       "          [ 0.3253,  0.3178, -0.3321]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2780,  0.2376,  0.2493],\n",
       "          [ 0.0325,  0.2731, -0.1962],\n",
       "          [-0.1873, -0.1788, -0.2564]]],\n",
       "\n",
       "\n",
       "        [[[-0.0938,  0.1865,  0.3379],\n",
       "          [-0.2345, -0.1432,  0.2664],\n",
       "          [-0.1341, -0.2131,  0.0141]]],\n",
       "\n",
       "\n",
       "        [[[-0.2515, -0.0585, -0.0160],\n",
       "          [-0.2950,  0.3166,  0.2234],\n",
       "          [-0.3188, -0.2318, -0.3022]]],\n",
       "\n",
       "\n",
       "        [[[-0.2976, -0.3562,  0.1439],\n",
       "          [ 0.1476,  0.1382,  0.0263],\n",
       "          [-0.2634, -0.0442,  0.3444]]],\n",
       "\n",
       "\n",
       "        [[[-0.1823,  0.2918,  0.1068],\n",
       "          [-0.3444, -0.2273,  0.0849],\n",
       "          [-0.0839, -0.0244,  0.1695]]],\n",
       "\n",
       "\n",
       "        [[[-0.1371,  0.2729,  0.1520],\n",
       "          [-0.2668,  0.0292,  0.3027],\n",
       "          [-0.1455,  0.2440,  0.1145]]],\n",
       "\n",
       "\n",
       "        [[[-0.0032,  0.2156, -0.2727],\n",
       "          [-0.3135, -0.0952,  0.3329],\n",
       "          [ 0.0662,  0.3106,  0.0016]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2480, -0.3041, -0.1946],\n",
       "          [-0.1903,  0.0321, -0.2136],\n",
       "          [ 0.1593, -0.0577, -0.0891]]],\n",
       "\n",
       "\n",
       "        [[[-0.3306, -0.2609, -0.0518],\n",
       "          [-0.1150,  0.1618, -0.3160],\n",
       "          [ 0.0656,  0.0547,  0.1441]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1969, -0.3256, -0.1003],\n",
       "          [ 0.1446,  0.2107, -0.3239],\n",
       "          [-0.3070, -0.0190,  0.3058]]],\n",
       "\n",
       "\n",
       "        [[[-0.0253,  0.1025, -0.3250],\n",
       "          [ 0.2419,  0.1577,  0.1932],\n",
       "          [-0.0425, -0.1092,  0.0434]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2008,  0.1035,  0.0932],\n",
       "          [-0.1293, -0.2032, -0.2236],\n",
       "          [ 0.2643, -0.3384, -0.0742]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1467, -0.1769, -0.0673],\n",
       "          [-0.2178, -0.1425, -0.1762],\n",
       "          [ 0.0900,  0.2337, -0.2931]]],\n",
       "\n",
       "\n",
       "        [[[-0.1603, -0.0445, -0.1491],\n",
       "          [ 0.1885,  0.0391,  0.0120],\n",
       "          [ 0.2439,  0.2301, -0.1322]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1151, -0.3292,  0.0758],\n",
       "          [-0.0853,  0.3195, -0.3186],\n",
       "          [ 0.1751,  0.0545, -0.1005]]],\n",
       "\n",
       "\n",
       "        [[[-0.2315, -0.3145,  0.2052],\n",
       "          [ 0.2209,  0.1948,  0.2200],\n",
       "          [-0.3051, -0.1262, -0.0961]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0426,  0.0124, -0.3309],\n",
       "          [-0.0994,  0.2747,  0.2284],\n",
       "          [ 0.1831, -0.1076,  0.2249]]],\n",
       "\n",
       "\n",
       "        [[[-0.0980,  0.0456, -0.2642],\n",
       "          [-0.1910,  0.1692,  0.0131],\n",
       "          [ 0.1695,  0.0574, -0.2990]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3332, -0.1310, -0.1268],\n",
       "          [-0.1319, -0.2086, -0.3498],\n",
       "          [-0.1308, -0.1598,  0.1139]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2974, -0.2887,  0.1774],\n",
       "          [ 0.0428, -0.2496, -0.0929],\n",
       "          [ 0.2444, -0.0185, -0.0611]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1337,  0.0145,  0.1205],\n",
       "          [-0.0317, -0.1852,  0.1207],\n",
       "          [ 0.2605, -0.1312,  0.3093]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2492,  0.0282,  0.0063],\n",
       "          [ 0.1437,  0.0261, -0.2930],\n",
       "          [ 0.1109,  0.0644, -0.1777]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2627, -0.2902,  0.0290],\n",
       "          [-0.2390,  0.0822,  0.0898],\n",
       "          [-0.1514, -0.2665,  0.1504]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2720,  0.3230, -0.2366],\n",
       "          [ 0.0660,  0.1996, -0.1970],\n",
       "          [ 0.3371, -0.0272,  0.0059]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0376,  0.1845,  0.2735],\n",
       "          [-0.1163, -0.2263, -0.3142],\n",
       "          [-0.1483, -0.3401, -0.0332]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2367,  0.2019,  0.1198],\n",
       "          [-0.1971, -0.2607, -0.0770],\n",
       "          [ 0.1484, -0.0534, -0.2204]]],\n",
       "\n",
       "\n",
       "        [[[-0.0797,  0.2436, -0.1057],\n",
       "          [ 0.3403, -0.1532, -0.3622],\n",
       "          [ 0.2706, -0.2145, -0.3234]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3005,  0.0390, -0.2851],\n",
       "          [-0.0720, -0.0413, -0.1333],\n",
       "          [ 0.0423, -0.3177,  0.0320]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2790,  0.1830,  0.0170],\n",
       "          [-0.0364, -0.3033,  0.0399],\n",
       "          [-0.3121,  0.1457,  0.0018]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2750,  0.0242,  0.2884],\n",
       "          [ 0.1198,  0.0360,  0.2823],\n",
       "          [ 0.2432, -0.2141, -0.1477]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2338,  0.3040, -0.2912],\n",
       "          [ 0.1508, -0.0934,  0.1439],\n",
       "          [-0.0446, -0.2499,  0.1262]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2478, -0.1665, -0.0253],\n",
       "          [-0.1939,  0.3200,  0.2044],\n",
       "          [-0.0170,  0.1432, -0.2661]]],\n",
       "\n",
       "\n",
       "        [[[-0.1497,  0.3111, -0.0756],\n",
       "          [-0.0078,  0.2728,  0.0107],\n",
       "          [-0.1111,  0.0530,  0.3326]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0949,  0.2815, -0.2209],\n",
       "          [-0.0740,  0.2305, -0.2007],\n",
       "          [-0.0883, -0.3377, -0.1908]]],\n",
       "\n",
       "\n",
       "        [[[-0.3248, -0.1827, -0.1184],\n",
       "          [-0.1967,  0.0440, -0.2999],\n",
       "          [-0.0009, -0.3186, -0.0193]]],\n",
       "\n",
       "\n",
       "        [[[-0.2234,  0.3359,  0.2561],\n",
       "          [ 0.0503,  0.3108, -0.3050],\n",
       "          [-0.2678,  0.0935,  0.0739]]],\n",
       "\n",
       "\n",
       "        [[[-0.1202,  0.3101, -0.2631],\n",
       "          [-0.3514, -0.1139,  0.1193],\n",
       "          [-0.1877,  0.0590, -0.2822]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2385, -0.1756, -0.3193],\n",
       "          [-0.0559, -0.2286, -0.3099],\n",
       "          [ 0.2227,  0.1435,  0.0432]]],\n",
       "\n",
       "\n",
       "        [[[-0.1803, -0.3527,  0.1537],\n",
       "          [-0.1989, -0.3171,  0.3099],\n",
       "          [-0.2762, -0.0071,  0.3104]]],\n",
       "\n",
       "\n",
       "        [[[-0.1318,  0.2996, -0.2852],\n",
       "          [ 0.1585, -0.0190, -0.1182],\n",
       "          [ 0.0220, -0.1145, -0.2962]]],\n",
       "\n",
       "\n",
       "        [[[-0.1790,  0.1244,  0.0827],\n",
       "          [-0.2513, -0.0719,  0.3301],\n",
       "          [-0.0323, -0.1124, -0.2088]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0861,  0.1902,  0.2297],\n",
       "          [-0.0178, -0.0079,  0.0240],\n",
       "          [-0.0427,  0.2270,  0.2594]]]], requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.features[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be27ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
